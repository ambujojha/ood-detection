{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_px(ppl, lls):\n",
    "    lengths = np.array([len(ll) for ll in lls])\n",
    "    logpx = np.log(ppl) * lengths * -1\n",
    "    return logpx\n",
    "\n",
    "def compute_auroc_all(id_msp, id_px, id_ppl, ood_msp, ood_px, ood_ppl, do_print=False):\n",
    "    score_px = compute_auroc(-id_px, -ood_px)\n",
    "    score_py = compute_auroc(-id_msp, -ood_msp)\n",
    "    score_ppl = compute_auroc(id_ppl, ood_ppl)\n",
    "    if do_print:\n",
    "        print(f\"P(x): {score_px:.3f}\")\n",
    "        print(f\"P(y | x): {score_py:.3f}\")\n",
    "        print(f\"Perplexity: {score_ppl:.3f}\")\n",
    "    scores = {\n",
    "        'p_x': score_px,\n",
    "        'p_y': score_py,\n",
    "        'ppl': score_ppl\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_auroc(id_pps, ood_pps, normalize=False, return_curve=False):\n",
    "    y = np.concatenate((np.ones_like(ood_pps), np.zeros_like(id_pps)))\n",
    "    scores = np.concatenate((ood_pps, id_pps))\n",
    "    if normalize:\n",
    "        scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "    if return_curve:\n",
    "        return roc_curve(y, scores)\n",
    "    else:\n",
    "        return 100*roc_auc_score(y, scores)\n",
    "\n",
    "def compute_far(id_pps, ood_pps, rate=5):\n",
    "    incorrect = len(id_pps[id_pps > np.percentile(ood_pps, rate)])\n",
    "    return 100*incorrect / len(id_pps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_metric_all(id_msp, id_px, id_ppl, ood_msp, ood_px, ood_ppl, metric='auroc', do_print=False):\n",
    "    if metric == 'auroc':\n",
    "        score_px = compute_auroc(-id_px, -ood_px)\n",
    "        score_py = compute_auroc(-id_msp, -ood_msp)\n",
    "        score_ppl = compute_auroc(id_ppl, ood_ppl)\n",
    "    elif metric == 'far':\n",
    "        score_px = compute_far(-id_px, -ood_px)\n",
    "        score_py = compute_far(-id_msp, -ood_msp)\n",
    "        score_ppl = compute_far(id_ppl, ood_ppl)\n",
    "    else:\n",
    "        raise Exception('Invalid metric name')\n",
    "\n",
    "    if do_print:\n",
    "        print(f\"Metric {metric}:\")\n",
    "        print(f\"P(x): {score_px:.3f}\")\n",
    "        print(f\"P(y | x): {score_py:.3f}\")\n",
    "        print(f\"Perplexity: {score_ppl:.3f}\\n\")\n",
    "\n",
    "    scores = {\n",
    "        'p_x': score_px,\n",
    "        'p_y': score_py,\n",
    "        'ppl': score_ppl\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def read_model_out(fname):\n",
    "    ftype = fname.split('.')[1]\n",
    "    \n",
    "    if ftype == 'pkl':\n",
    "        with open(fname, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    elif ftype == 'npy':\n",
    "        return np.load(fname)\n",
    "    else:\n",
    "        raise KeyError(f'{ftype} not supported')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willi\\Documents\\NYU\\2020_Fall\\nlp\\project\\ood-detection\n"
     ]
    }
   ],
   "source": [
    "repo = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "print(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(repo, 'output')\n",
    "fig_dir = os.path.join(repo, 'figs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sets = ['imdb', 'sst2']\n",
    "eval_sets = ['imdb', 'sst2', 'snli', 'counterfactual-imdb']\n",
    "methods = ['msp', 'lls', 'pps']\n",
    "\n",
    "signals = {}\n",
    "for train_set in train_sets:\n",
    "    for eval_set in eval_sets:\n",
    "        signals[(train_set, eval_set)] = {method: None for method in methods}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "method2ftype={\n",
    "    'msp': 'npy',\n",
    "    'lls': 'pkl',\n",
    "    'pps': 'npy',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('imdb', 'snli', 'lls')\n",
      "('imdb', 'snli', 'pps')\n",
      "('imdb', 'counterfactual-imdb', 'lls')\n",
      "('imdb', 'counterfactual-imdb', 'pps')\n",
      "('sst2', 'snli', 'lls')\n",
      "('sst2', 'snli', 'pps')\n",
      "('sst2', 'counterfactual-imdb', 'lls')\n",
      "('sst2', 'counterfactual-imdb', 'pps')\n"
     ]
    }
   ],
   "source": [
    "best_lr = {\n",
    "    'imdb': '5e-5',\n",
    "    'sst2': '5e-5',\n",
    "}\n",
    "\n",
    "methods = ['lls', 'pps']\n",
    "not_readys = []\n",
    "\n",
    "for (train_set, eval_set), signals_dict in signals.items():\n",
    "    for method in methods:\n",
    "        signal_fname = os.path.join(output_dir, 'gpt2', train_set, f'{eval_set}_{best_lr[train_set]}_{method}.{method2ftype[method]}')\n",
    "        if not os.path.exists(signal_fname):\n",
    "            not_readys.append((train_set, eval_set, method))\n",
    "            continue\n",
    "\n",
    "        signals_dict[method] = read_model_out(signal_fname)\n",
    "        \n",
    "for not_ready in not_readys:\n",
    "    print(not_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "methods = ['msp']\n",
    "not_readys = []\n",
    "\n",
    "model_type = 'roberta-large'\n",
    "\n",
    "for (train_set, eval_set), signals_dict in signals.items():\n",
    "    for method in methods:\n",
    "        signal_fname = os.path.join(output_dir, 'roberta', train_set, f'{model_type}_{eval_set}_{method}.{method2ftype[method]}')\n",
    "        if not os.path.exists(signal_fname):\n",
    "            not_readys.append((train_set, eval_set, method))\n",
    "            continue\n",
    "\n",
    "        signals_dict[method] = read_model_out(signal_fname)\n",
    "        \n",
    "for not_ready in not_readys:\n",
    "    print(not_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Method      score in_domain out_domain metric\n",
      "0   GPT2: $p(x)$   0.410715      IMDB      SST-2  AUROC\n",
      "1   RoBERTa: MSP  70.281889      IMDB      SST-2  AUROC\n",
      "2      GPT2: PPL  91.266848      IMDB      SST-2  AUROC\n",
      "3   GPT2: $p(x)$  99.988000      IMDB      SST-2  FAR95\n",
      "4   RoBERTa: MSP  80.905000      IMDB      SST-2  FAR95\n",
      "5      GPT2: PPL  50.696000      IMDB      SST-2  FAR95\n",
      "6   GPT2: $p(x)$  99.948033     SST-2       IMDB  AUROC\n",
      "7   RoBERTa: MSP  64.283467     SST-2       IMDB  AUROC\n",
      "8      GPT2: PPL  90.089902     SST-2       IMDB  AUROC\n",
      "9   GPT2: $p(x)$   0.000000     SST-2       IMDB  FAR95\n",
      "10  RoBERTa: MSP  70.343840     SST-2       IMDB  FAR95\n",
      "11     GPT2: PPL  34.486546     SST-2       IMDB  FAR95\n"
     ]
    }
   ],
   "source": [
    "metrics = ['auroc', 'far']\n",
    "\n",
    "metric_summary = []\n",
    "\n",
    "score2plot = {\n",
    "    'p_x': r'GPT2: $p(x)$',\n",
    "    'ppl': 'GPT2: PPL',\n",
    "    'p_y': 'RoBERTa: MSP',\n",
    "}\n",
    "\n",
    "metric2plot = {\n",
    "    'auroc': 'AUROC',\n",
    "    'far': 'FAR95'\n",
    "}\n",
    "\n",
    "dataset2plot = {\n",
    "    'imdb': 'IMDB',\n",
    "    'sst2': 'SST-2',\n",
    "    'snli': 'SNLI',\n",
    "    'counterfactual-imdb': 'cIMDB',\n",
    "}\n",
    "\n",
    "not_ready = []\n",
    "for train_set in train_sets:\n",
    "    for eval_set in eval_sets:\n",
    "        if train_set == eval_set:\n",
    "            continue\n",
    "        \n",
    "        ood_signal_dict = signals[(train_set, eval_set)]\n",
    "        id_signal_dict = signals[(train_set, train_set)]\n",
    "        \n",
    "        skip=False\n",
    "        for value in ood_signal_dict.values():\n",
    "            if isinstance(value, type(None)):\n",
    "                skip=True\n",
    "                \n",
    "        if skip:\n",
    "            not_ready.append((train_set, eval_set))\n",
    "            continue\n",
    "            \n",
    "        ood_px = compute_px(ood_signal_dict['pps'], ood_signal_dict['lls'])\n",
    "        id_px = compute_px(id_signal_dict['pps'], id_signal_dict['lls'])\n",
    "               \n",
    "        for metric in metrics:\n",
    "            scores = compute_metric_all(\n",
    "                id_msp=id_signal_dict['msp'],\n",
    "                id_px=id_px,\n",
    "                id_ppl=id_signal_dict['pps'],\n",
    "                ood_msp=ood_signal_dict['msp'],\n",
    "                ood_px=ood_px,\n",
    "                ood_ppl=ood_signal_dict['pps'],\n",
    "                metric=metric,\n",
    "                do_print=verbose\n",
    "            )\n",
    "            \n",
    "            for score_key, score_value in scores.items():\n",
    "                row = {'Method': score2plot[score_key], 'score': score_value}\n",
    "            \n",
    "                row['in_domain'] = dataset2plot[train_set]\n",
    "                row['out_domain'] = dataset2plot[eval_set]\n",
    "                row['metric'] = metric2plot[metric]\n",
    "\n",
    "                metric_summary.append(row)\n",
    "\n",
    "print(pd.DataFrame(metric_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_size=24\n",
    "label_size=18\n",
    "annot_size=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_plot(\n",
    "    df=None,\n",
    "    x='out_domain',\n",
    "    y='score',\n",
    "    hue='Method',\n",
    "    title=None,\n",
    "    xlabel='',\n",
    "    ylabel='',\n",
    "    palette='colorblind',\n",
    "    ylim=[0,110],\n",
    "    yticks=np.arange(0,110,20),\n",
    "    order=None,\n",
    "    val_offset = 5,\n",
    "    hue_order=None,\n",
    "    figsize=(10,5)\n",
    "):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    splot = sns.barplot(\n",
    "        data=df, \n",
    "        x=x, \n",
    "        y=y,\n",
    "        hue=hue,\n",
    "        palette=palette,\n",
    "        hue_order=hue_order,\n",
    "    )\n",
    "    \n",
    "    for p in splot.patches:\n",
    "        splot.annotate(\n",
    "            f'{p.get_height():.1f}',\n",
    "            (p.get_x() + p.get_width() / 2, p.get_height() + val_offset),\n",
    "            ha='center', va='center',\n",
    "            fontsize=annot_size\n",
    "        )\n",
    "    \n",
    "    if not title is None:\n",
    "        ax.set_title(title, fontsize=title_size)\n",
    "        \n",
    "    ax.set_xlabel(xlabel, fontsize=label_size)\n",
    "    ax.set_ylabel(ylabel, fontsize=label_size)\n",
    "        \n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "    splot.set_yticklabels(yticks, size=annot_size)\n",
    "    splot.set_xticklabels(order, size=annot_size)\n",
    "    \n",
    "    fig.tight_layout()  \n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willi\\Anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976bbdb0325a422192f70559dbf028ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willi\\Anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10326dd9ea9f4930aded5ca450222de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willi\\Anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c4c2c2a1cc407ca863509381ec5bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willi\\Anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7b2970916e4840a67bc25d459118c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary_df = pd.DataFrame(metric_summary)\n",
    "xlabel = 'Out-of-Domain Sets'\n",
    "\n",
    "order_all = ['cIMDB', 'IMDB', 'SST-2', 'SNLI']\n",
    "hue_order = ['RoBERTa: MSP',  'GPT2: PPL', r'GPT2: $p(x)$']\n",
    "palette = {\n",
    "    'RoBERTa: MSP':'tab:orange',\n",
    "    'GPT2: PPL':'tab:cyan',\n",
    "    r'GPT2: $p(x)$':'tab:blue',\n",
    "}\n",
    "figsize = (10, 5)\n",
    "\n",
    "save = True\n",
    "fig_ftype = 'jpg'\n",
    "\n",
    "for metric in summary_df['metric'].unique():\n",
    "    sub_df = summary_df.loc[summary_df['metric'] == metric, :]\n",
    "    \n",
    "    for in_domain in sub_df['in_domain'].unique():\n",
    "        subsub_df = sub_df.loc[sub_df['in_domain'] == in_domain, :]\n",
    "        \n",
    "        order=[out_domain for out_domain in order_all if out_domain in subsub_df['out_domain'].unique()]\n",
    "        \n",
    "        fig = my_plot(\n",
    "            df=subsub_df,\n",
    "#             title=f'{in_domain}',\n",
    "            xlabel=xlabel,\n",
    "#             ylabel=metric,\n",
    "            order=order,\n",
    "            hue_order=hue_order,\n",
    "            palette=palette,\n",
    "            figsize=figsize,\n",
    "        )\n",
    "        \n",
    "        if save:\n",
    "            os.makedirs(fig_dir, exist_ok=True)\n",
    "            fig.savefig(os.path.join(fig_dir, f'{in_domain}_{metric}.{fig_ftype}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Legacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### IMDB as ID vs SST-2 as OOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "ppl_base_path = '../output/gpt2/sst2/'\n",
    "\n",
    "imdb_pps = np.load(ppl_base_path + 'imdb_5e-5_pps.npy')\n",
    "sst2_pps = np.load(ppl_base_path + 'sst2_5e-5_pps.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "with open(ppl_base_path + 'imdb_5e-5_lls.pkl', 'rb') as f:\n",
    "    imdb_lls = pickle.load(f)\n",
    "\n",
    "with open(ppl_base_path + 'sst2_5e-5_lls.pkl', 'rb') as f:\n",
    "    sst2_lls = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "msp_base_path = '../roberta/msp/'\n",
    "\n",
    "imdb_msp = np.load(msp_base_path + 'large_imdb_msp.npy')\n",
    "# imdb_msp = np.load(msp_base_path + 'textattack_imdb_msp.npy')\n",
    "sst2_msp = np.load(msp_base_path + 'large_sst2_msp.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "all_pps = {\n",
    "    'imdb': imdb_pps,\n",
    "#     'yelp': yelp_pps,\n",
    "    'sst2': sst2_pps,\n",
    "#     'snli': snli_pps,\n",
    "#     'rte': rte_pps\n",
    "}\n",
    "\n",
    "all_lls = {\n",
    "    'imdb': imdb_lls,\n",
    "#     'yelp': yelp_lls,\n",
    "    'sst2': sst2_lls,\n",
    "#     'snli': snli_lls,\n",
    "#     'rte': rte_lls\n",
    "}\n",
    "\n",
    "all_msp = {\n",
    "    'imdb': imdb_msp,\n",
    "#     'yelp': yelp_msp,\n",
    "    'sst2': sst2_msp,\n",
    "#     'snli': snli_msp,\n",
    "#     'rte': rte_msp\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "all_px, all_pxy = {}, {}\n",
    "for ds in all_pps.keys():\n",
    "    all_px[ds] = compute_px(all_pps[ds], all_lls[ds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "dataset_pairs = [('sst2', 'imdb')]\n",
    "metrics = ['auroc', 'far']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "for (id_name, ood_name) in dataset_pairs:\n",
    "    print(f\"-------{id_name} vs {ood_name}-------\")\n",
    "    for metric in metrics:\n",
    "        compute_metric_all(all_msp[id_name], all_px[id_name],\n",
    "                           all_pps[id_name], all_msp[ood_name],\n",
    "                           all_px[ood_name], all_pps[ood_name], metric=metric, do_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "results = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    results[metric] = {}\n",
    "    for (id_name, ood_name) in dataset_pairs:\n",
    "        results[metric][f'{id_name}-{ood_name}'] = compute_metric_all(all_msp[id_name], all_px[id_name],\n",
    "                           all_pps[id_name], all_msp[ood_name],\n",
    "                           all_px[ood_name], all_pps[ood_name], metric=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "all_dfs = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    all_dfs[metric] = pd.DataFrame.from_dict(results[metric], orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"AUROC:\")\n",
    "\n",
    "all_dfs['auroc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"FAR95:\")\n",
    "\n",
    "all_dfs['far']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
