{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(data_file):\n",
    "    data = [json.loads(l) for l in open(data_file, \"r\")]\n",
    "    return data\n",
    "\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    #return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "    return white_space_fix(remove_punc(lower(s)))\n",
    "\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s: return []\n",
    "    return normalize_text(s).split()\n",
    "\n",
    "\n",
    "def count_ngrams(txt, max_n):\n",
    "    out_d = {}\n",
    "    for n in range(1, max_n + 1):\n",
    "        ngrams = zip(*[txt[i:] for i in range(n)])\n",
    "        ngram2count = Counter(ngrams)\n",
    "        out_d[n] = ngram2count\n",
    "    return out_d\n",
    "\n",
    "def get_ngram_counts(exs, fields, max_n=4, cache_out = None, fname = None, overwrite=False):    \n",
    "    ngrams = []\n",
    "    ngrams2count = defaultdict(Counter)\n",
    "    \n",
    "    if (not cache_out is None) and not overwrite and os.path.exists(cache_out):\n",
    "        ngrams2count, ngrams = pkl.load(open(cache_out, 'rb'))\n",
    "        print(f'Loaded cached ngram counts from {cache_out}')\n",
    "    else:\n",
    "        for ex in tqdm(exs, desc=f'Counting n-grams {fname}'):\n",
    "            field2ngrams = {}\n",
    "            for field in fields:\n",
    "                field_toks = get_tokens(ex[field])\n",
    "                field_ngrams = count_ngrams(field_toks, max_n)\n",
    "                field2ngrams[field] = field_ngrams\n",
    "                for n, count in field_ngrams.items():\n",
    "                    ngrams2count[n] += count\n",
    "            ngrams.append(field2ngrams)\n",
    "\n",
    "        if not cache_out is None:\n",
    "            pkl.dump((ngrams2count,ngrams), open(cache_out, 'wb'))\n",
    "            print(f'Saved ngram counts to {cache_out}')\n",
    "        \n",
    "    return (ngrams2count,ngrams)\n",
    "\n",
    "def get_length_stats(exs):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_base = os.path.join('.', 'ngram_stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "id_val_imdb:   0%|                                                                               | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zip argument #2 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-1662c84530ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtemp_ex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mex_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mexs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_ex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     ngram_counts[data_key] = get_ngram_counts(\n",
      "\u001b[1;31mTypeError\u001b[0m: zip argument #2 must support iteration"
     ]
    }
   ],
   "source": [
    "exs, ngram_counts, unigram_lengths = {}, {}, {}\n",
    "\n",
    "with open(os.path.join('.', 'all_val_data.p'), 'rb') as f:\n",
    "    val_data = pkl.load(f)    \n",
    "\n",
    "t = tqdm(val_data.items())\n",
    "for data_key, ex_dict in t:\n",
    "    t.set_description('_'.join(data_key))\n",
    "    print(ex_dict['label'])\n",
    "    temp_ex = [{'text': text, 'label': label} for text, label in zip(ex_dict['text'], ex_dict['label'])]\n",
    "    exs[data_key] = temp_ex\n",
    "    ngram_counts[data_key] = get_ngram_counts(\n",
    "        temp_ex,\n",
    "        ['text', 'label'],\n",
    "        cache_out = os.path.join(cache_base, 'counts', f\"{'_'.join(data_key)}_counts.p\"),\n",
    "        fname = data_key\n",
    "    )\n",
    "    unigram_lengths[data_key] = [count_grams(text, 1) for text in ex_dict['text']]\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
